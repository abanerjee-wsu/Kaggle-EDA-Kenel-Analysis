{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load library to make headless http calls\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "# TODO\n",
    "# author location - done\n",
    "# get package information - done\n",
    "# get API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_subset(kernels):\n",
    "    dataset = []\n",
    "    \n",
    "    for kernel in kernels:\n",
    "    # get only the data we need\n",
    "        subset = {\n",
    "            'displayName': kernel['author']['displayName'],\n",
    "            'userId': kernel['author']['userId'],\n",
    "            'profileUrl': kernel['author']['profileUrl'], # will be used later to get location data\n",
    "            'id': kernel['id'],\n",
    "            'title': kernel['title'],\n",
    "            'totalVotes': kernel['totalVotes'], # used for rating sort\n",
    "            'languageName' : kernel['languageName'],\n",
    "            'scriptUrl' : kernel['scriptUrl'],\n",
    "            'scriptVersionDateCreated' : kernel['scriptVersionDateCreated']\n",
    "        }\n",
    "        \n",
    "        # append the subset to the dataset\n",
    "        dataset.append(subset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# total number of entries to request\n",
    "request_size = 4500\n",
    "\n",
    "# size of each http request\n",
    "page_size = 1000\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    # create empty list to store data\n",
    "    master_dataset = []\n",
    "    \n",
    "    \n",
    "    for n in range(0,request_size,page_size):\n",
    "        url = 'https://www.kaggle.com/kernels.json?sortBy=hotness&group=everyone&pageSize={0}&language=all&outputType=all&isMixedPrivacyLayout=true&startRow={1}'.format(page_size,n)\n",
    "        print(url)\n",
    "        page = requests.get(url) \n",
    "        kernels = json.loads(page.text)\n",
    "\n",
    "        # print size of request return\n",
    "        print(len(kernels))\n",
    "\n",
    "        master_dataset = master_dataset + return_subset(kernels)\n",
    "\n",
    "        # write dictionary to JSON\n",
    "        with open('kernels.json', 'w') as fp:\n",
    "            json.dump(master_dataset, fp)\n",
    "\n",
    "        # write dictionary to CSV\n",
    "        with open('kernels.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
    "            w = csv.DictWriter(f, master_dataset[0].keys())\n",
    "            w.writeheader()\n",
    "            w.writerows(master_dataset)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'Lisbon', 'country': 'PT'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get location information from author profile page\n",
    "def get_author_location(profile_url):\n",
    "    url = 'https://www.kaggle.com{0}'.format(profile_url)\n",
    "    \n",
    "    # make http request for page\n",
    "    page = requests.get(url) \n",
    "    \n",
    "    # find location key value pairs in source using regex\n",
    "    match_object = re.findall( r'\\\"(country|region|city)\\\":\\\"(.*?)\\\"', page.text, re.I|re.M)\n",
    "    \n",
    "    # turn match object into dictionary to be merged into kernel object\n",
    "    location_data = { match[0]:match[1] for match in match_object }\n",
    "    \n",
    "    #example: {'country': 'United Kingdom', 'region': 'England', 'city': 'London'}\n",
    "    \n",
    "    return location_data\n",
    "\n",
    "get_author_location('/pmarcelino')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stats',\n",
       " 'norm',\n",
       " 'warnings',\n",
       " 'StandardScaler',\n",
       " 'seaborn',\n",
       " 'matplotlib',\n",
       " 'numpy',\n",
       " 'pandas']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get package list from source code of kernel\n",
    "# does not look for combined import statements\n",
    "\n",
    "def get_package_list(code_url):\n",
    "    url = 'https://www.kaggle.com{0}/code'.format(code_url)\n",
    "    \n",
    "    # make http request for page\n",
    "    page = requests.get(url) \n",
    "    \n",
    "    # find import references using regex\n",
    "    match_object = re.findall( r'import (\\w*)', page.text, re.I|re.M)\n",
    "    \n",
    "    # get unique entries\n",
    "    package_list = list(set(match_object))\n",
    "    \n",
    "    # filter out non-package names\n",
    "    filter_words = ['the']\n",
    "    package_list = list(set(package_list) - set(filter_words))\n",
    "    \n",
    "    return package_list\n",
    "    \n",
    "get_package_list('/pmarcelino/comprehensive-data-exploration-with-python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numpy', 1219),\n",
       " ('pandas', 1211),\n",
       " ('matplotlib', 794),\n",
       " ('seaborn', 526),\n",
       " ('ggplot2', 319),\n",
       " ('check_output', 302),\n",
       " ('dplyr', 260),\n",
       " ('xgboost', 256),\n",
       " ('train_test_split', 235),\n",
       " ('', 206)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a count of all packages\n",
    "\n",
    "conn=sqlite3.connect(\"Kernels_db.db\")\n",
    "\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT * FROM Kernels\")\n",
    "results=c.fetchall()\n",
    "\n",
    "all_packages = []\n",
    "\n",
    "for row in results:\n",
    "    packages = dict(row)['packages'].split(\",\")\n",
    "    all_packages = all_packages + packages\n",
    "\n",
    "    \n",
    "packages_count = collections.Counter(all_packages)\n",
    "\n",
    "packages_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only get python code\n",
    "# look for only top 5 libraries\n",
    "# methods (api) calls two levels deep\n",
    "# deficiencies: possibility of inaccurate class/method counts due to name overlaps between libraries\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import nbformat\n",
    "import io\n",
    "import tokenize\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "def strip_comments(code):\n",
    "    code = str(code)\n",
    "    return re.sub(r'(?m)^ *#.*\\n?', '', code)\n",
    "   \n",
    "def get_source_code(url):\n",
    "    \n",
    "    # read notebook from url\n",
    "    response = urlopen(url).read().decode()\n",
    "\n",
    "    # read http response into nbconvert object\n",
    "    notebook = nbformat.reads(response, as_version=4)\n",
    "    \n",
    "    # instantiate PythonExporter  \n",
    "    py_exporter = PythonExporter()\n",
    "\n",
    "    # convert notebook into python source (ignoring markup, etc)\n",
    "    script, resources = py_exporter.from_notebook_node(notebook)\n",
    "    \n",
    "    # remove all comments from source\n",
    "    raw_script = stripComments(script)\n",
    "    \n",
    "    return raw_script\n",
    "\n",
    "url = \"https://kagglesds.blob.core.windows.net/script-versions/1766385/notebook/__notebook__.ipynb?sv=2015-12-11&sr=b&sig=ytykV0edLA8D93%2Fn5eNntNI5%2Fd8q9aI%2F9%2F2Rmxx0aQw%3D&se=2017-11-18T03%3A41%3A04Z&sp=r\"\n",
    "\n",
    "code = get_source_code(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'lmplot': 1, 'pairplot': 1, 'regplot': 3})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "#read file for testing\n",
    "py_source_test = open(\"test.py\",'r').read()\n",
    "\n",
    "# get class names for a library\n",
    "def get_library_classes(name):\n",
    "    names = dir(eval(name))    \n",
    "    class_names = list(filter(lambda x: x[0].isupper(), names))\n",
    "    return class_names\n",
    "\n",
    "# get method names for a library or class\n",
    "def get_library_methods(name):\n",
    "    names = dir(eval(name))    \n",
    "    class_names = list(filter(lambda x: x[0].islower(), names))\n",
    "    return class_names\n",
    "\n",
    "# def search_for_methods(method_names):\n",
    "\n",
    "names = get_library_methods(\"seaborn\")\n",
    "\n",
    "\n",
    "match_object = re.findall( r'.(pairplot|lmplot|regplot)\\(', py_source_test, re.I|re.M)\n",
    "\n",
    "\n",
    "match_object\n",
    "\n",
    "collections.Counter(match_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
